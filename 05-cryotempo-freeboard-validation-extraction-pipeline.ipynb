{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4124ff1",
   "metadata": {},
   "source": [
    "# Cryo-TEMPO NetCDF Dataset Structure and Variable Inspection\n",
    "\n",
    "## Overview\n",
    "This notebook inspects the structure and variables of the **first observational file** from the Cryo-TEMPO sea ice product for **July 2010**. The Cryo-TEMPO (CryoSat-2 Thematic Exploitation Polar Observation) product provides along-track sea ice freeboard measurements over the Antarctic region.\n",
    "\n",
    "### Dataset Information\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **File** | `CS_OFFL_SIR_TDP_SI_ANTARC_20100716T000456_20100716T000635_02_03041_C001.nc` |\n",
    "| **Date** | July 16, 2010 |\n",
    "| **Region** | Antarctic |\n",
    "| **Product Type** | TDP_SI (Thematic Data Product - Sea Ice) |\n",
    "| **Orbit Number** | 03041 |\n",
    "\n",
    "### Objectives\n",
    "1. Load and validate the NetCDF file structure\n",
    "2. Extract and display global attributes (metadata)\n",
    "3. List all dimensions with their sizes\n",
    "4. Enumerate all variables with their properties (dtype, shape, units, description)\n",
    "5. Generate summary statistics for key numeric variables\n",
    "6. Identify data quality indicators and potential issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53decd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 20:27:17 - INFO - __main__ - File path validated: D:\\phd\\data\\CryoTEMPO\\2010\\07\\CS_OFFL_SIR_TDP_SI_ANTARC_20100716T000456_20100716T000635_02_03041_C001.nc\n",
      "2025-12-07 20:27:17 - INFO - __main__ - Starting full NetCDF inspection...\n",
      "2025-12-07 20:27:17 - INFO - __main__ - Loading NetCDF dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  CRYO-TEMPO NETCDF DATASET INSPECTION REPORT\n",
      "  Generated: 2025-12-07 20:27:17\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 20:27:18 - INFO - __main__ - Dataset loaded successfully.\n",
      "2025-12-07 20:27:18 - INFO - __main__ - Inspection completed in 0.52 seconds.\n",
      "2025-12-07 20:27:18 - INFO - __main__ - Dataset handles closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "1. FILE INFORMATION\n",
      "================================================================================\n",
      "     Property                                                                                                    Value\n",
      "    File Name                               CS_OFFL_SIR_TDP_SI_ANTARC_20100716T000456_20100716T000635_02_03041_C001.nc\n",
      "    File Path D:\\phd\\data\\CryoTEMPO\\2010\\07\\CS_OFFL_SIR_TDP_SI_ANTARC_20100716T000456_20100716T000635_02_03041_C001.nc\n",
      "    File Size                                                                                0.0979 MB (102,706 bytes)\n",
      "Last Modified                                                                                      2024-07-13 20:34:24\n",
      "NetCDF Format                                                                                                  NETCDF4\n",
      "\n",
      "================================================================================\n",
      "2. GLOBAL ATTRIBUTES (METADATA)\n",
      "================================================================================\n",
      "              Attribute                                                                   Value\n",
      "   Metadata_Conventions                                          Unidata Dataset Discovery v1.0\n",
      "                  title                                     Cryo-TEMPO Sea Ice Thematic Product\n",
      "                project                                                          ESA Cryo-TEMPO\n",
      "           creator_name                                                  ESA Cryo-TEMPO Project\n",
      "            creator_url                                     http://cryosat.mssl.ucl.ac.uk/tempo\n",
      "           date_created                                              2024-05-29T09:25:14.379220\n",
      "               platform                                                               CryoSat-2\n",
      "                 sensor                                                                   SIRAL\n",
      "                   zone                                                               Antarctic\n",
      "     geospatial_lat_min                                                                -67.7773\n",
      "     geospatial_lat_max                                                                -61.8681\n",
      "     geospatial_lon_min                                                               -169.6791\n",
      "     geospatial_lon_max                                                               -168.0954\n",
      "geospatial_vertical_min                                                                     0.0\n",
      "geospatial_vertical_max                                                                     0.0\n",
      "    time_coverage_start                                              2010-07-16T00:04:56.822356\n",
      "      time_coverage_end                                              2010-07-16T00:06:35.410762\n",
      "       product_baseline                                                                       C\n",
      "        product_version                                                                     001\n",
      "            Conventions                                                                  CF-1.8\n",
      "           cycle_number                                                                       2\n",
      "       rel_orbit_number                                                                    3041\n",
      "       abs_orbit_number                                                                    1428\n",
      "          cnes_subcycle                                                                      17\n",
      "             cnes_track                                                                     289\n",
      "          cdm_data_type                                                              Trajectory\n",
      "                history Wed May 29 09:25:14 2024: Product generated with pysiral version 0.10\\n\n",
      "             sw_version                                                                    0.10\n",
      "                    doi                                                     10.5270/CR2-e2dd631\n",
      "\n",
      "Total Global Attributes: 29\n",
      "\n",
      "================================================================================\n",
      "3. DIMENSIONS\n",
      "================================================================================\n",
      "Dimension  Size  Type\n",
      "     time  2141 Fixed\n",
      "\n",
      "Total Dimensions: 1\n",
      "\n",
      "================================================================================\n",
      "4. VARIABLES\n",
      "================================================================================\n",
      "                     Variable   DType   Shape Dimensions Units  Valid  NaN\n",
      "              instrument_mode    int8 (2141,)  ('time',)   N/A   2141    0\n",
      "              radar_freeboard float64 (2141,)  ('time',)     m    995 1146\n",
      "  radar_freeboard_uncertainty float32 (2141,)  ('time',)     m    995 1146\n",
      "                  region_code    int8 (2141,)  ('time',)     1   2141    0\n",
      "            sea_ice_freeboard float64 (2141,)  ('time',)     m    745 1396\n",
      "   sea_ice_freeboard_filtered float64 (2141,)  ('time',)     m    745 1396\n",
      "sea_ice_freeboard_uncertainty float32 (2141,)  ('time',)     m    766 1375\n",
      "                   snow_depth float64 (2141,)  ('time',)     m   1196  945\n",
      "       snow_depth_uncertainty float32 (2141,)  ('time',)     m   1512  629\n",
      "\n",
      "Total Variables: 9\n",
      "\n",
      "------------------------------------------------------------\n",
      "Coordinate Variables:\n",
      "------------------------------------------------------------\n",
      "Coordinate          DType   Shape         Units\n",
      "  latitude        float64 (2141,) degrees_north\n",
      " longitude        float64 (2141,)  degrees_east\n",
      "      time datetime64[ns] (2141,)           N/A\n",
      "\n",
      "================================================================================\n",
      "5. VARIABLE DETAILED ATTRIBUTES\n",
      "================================================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "Variable: instrument_mode\n",
      "------------------------------------------------------------\n",
      "  comment: The mode that the SIRAL instrument was in at each measurement. Eith...\n",
      "  flag_meanings: LRM SAR SARIn\n",
      "  flag_values: [1 2 3]\n",
      "  long_name: SIRAL instrument mode flag\n",
      "  valid_max: 3\n",
      "  valid_min: 1\n",
      "\n",
      "------------------------------------------------------------\n",
      "Variable: radar_freeboard\n",
      "------------------------------------------------------------\n",
      "  ancillary_variables: radar_freeboard_uncertainty\n",
      "  comment: radar freeboard is defined as the elevation based on the assumption...\n",
      "  long_name: Radar freeboard\n",
      "  units: m\n",
      "\n",
      "------------------------------------------------------------\n",
      "Variable: radar_freeboard_uncertainty\n",
      "------------------------------------------------------------\n",
      "  comment: algorithm uncertainty (error propagation) of the radar freeeboard r...\n",
      "  long_name: Radar freeboard uncertainty\n",
      "  units: m\n",
      "\n",
      "------------------------------------------------------------\n",
      "Variable: region_code\n",
      "------------------------------------------------------------\n",
      "  comment: Source: Northern hemisphere: A new regional mask for Arctic sea ice...\n",
      "  coverage_content_type: referenceInformation\n",
      "  flag_meanings: Undefined_Region Central_Arctic Beaufort_Sea Chukchi_Sea East_Siber...\n",
      "  flag_values: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  1...\n",
      "  long_name: region id\n",
      "  unit: 1\n",
      "  units: 1\n",
      "  valid_max: 104\n",
      "  valid_min: 0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Variable: sea_ice_freeboard\n",
      "------------------------------------------------------------\n",
      "  ancillary_variables: sea_ice_freeboard_uncertainty\n",
      "  comment: freeboard of the sea ice layer\n",
      "  long_name: freeboard of the sea ice layer\n",
      "  standard_name: sea_ice_freeboard\n",
      "  units: m\n",
      "\n",
      "------------------------------------------------------------\n",
      "Variable: sea_ice_freeboard_filtered\n",
      "------------------------------------------------------------\n",
      "  comment: Filtered with a 450km lowess filter\n",
      "  long_name: smoothed freeboard of the sea ice layer\n",
      "  standard_name: sea_ice_freeboard\n",
      "  units: m\n",
      "\n",
      "------------------------------------------------------------\n",
      "Variable: sea_ice_freeboard_uncertainty\n",
      "------------------------------------------------------------\n",
      "  comment: algorithm uncertainty (error propagation) of the sea ice freeeboard...\n",
      "  long_name: sea ice freeboard uncertainty\n",
      "  units: m\n",
      "\n",
      "------------------------------------------------------------\n",
      "Variable: snow_depth\n",
      "------------------------------------------------------------\n",
      "  ancillary_variables: snow_depth_uncertainty\n",
      "  comment: Snow depth interpolated from model\n",
      "  long_name: snow depth\n",
      "  standard_name: surface_snow_thickness\n",
      "  units: m\n",
      "\n",
      "------------------------------------------------------------\n",
      "Variable: snow_depth_uncertainty\n",
      "------------------------------------------------------------\n",
      "  comment: Uncertainty of the snow depth.\n",
      "  long_name: snow depth uncertainty\n",
      "  units: m\n",
      "\n",
      "================================================================================\n",
      "6. SUMMARY STATISTICS (Numeric Variables)\n",
      "================================================================================\n",
      "                     Variable       Min      Max      Mean     Median   Std Dev  Count\n",
      "              instrument_mode         2        2         2          2         0   2141\n",
      "              radar_freeboard  -2.78308 0.852399 -0.153296 -0.0529311   0.38587    995\n",
      "  radar_freeboard_uncertainty  0.101981 0.141421   0.11758   0.105526 0.0177389    995\n",
      "                  region_code       102      102       102        102         0   2141\n",
      "            sea_ice_freeboard -0.247443 0.883348 0.0542246  0.0590798  0.151342    745\n",
      "   sea_ice_freeboard_filtered  -0.22781  0.18075 0.0518204   0.054441  0.059033    745\n",
      "sea_ice_freeboard_uncertainty  0.103621 0.143144   0.11751   0.107074 0.0162469    766\n",
      "                   snow_depth     0.104    0.281  0.206817      0.215 0.0575728   1196\n",
      "       snow_depth_uncertainty     0.015    0.146 0.0740437      0.068 0.0318144   1512\n",
      "\n",
      "================================================================================\n",
      "7. DATA QUALITY SUMMARY\n",
      "================================================================================\n",
      "                     Variable  Total  Valid  Missing Missing %\n",
      "              radar_freeboard   2141    995     1146    53.53%\n",
      "  radar_freeboard_uncertainty   2141    995     1146    53.53%\n",
      "            sea_ice_freeboard   2141    745     1396    65.20%\n",
      "   sea_ice_freeboard_filtered   2141    745     1396    65.20%\n",
      "sea_ice_freeboard_uncertainty   2141    766     1375    64.22%\n",
      "                   snow_depth   2141   1196      945    44.14%\n",
      "       snow_depth_uncertainty   2141   1512      629    29.38%\n",
      "\n",
      "------------------------------------------------------------\n",
      "Data Quality Issues Detected:\n",
      "------------------------------------------------------------\n",
      "  ⚠ WARNING: radar_freeboard has 53.5% missing values\n",
      "  ⚠ WARNING: radar_freeboard_uncertainty has 53.5% missing values\n",
      "  ⚠ WARNING: sea_ice_freeboard has 65.2% missing values\n",
      "  ⚠ WARNING: sea_ice_freeboard_filtered has 65.2% missing values\n",
      "  ⚠ WARNING: sea_ice_freeboard_uncertainty has 64.2% missing values\n",
      "\n",
      "================================================================================\n",
      "Inspection completed in 0.52 seconds.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cryo-TEMPO NetCDF Dataset Inspector\n",
    "\n",
    "This module provides comprehensive inspection of Cryo-TEMPO sea ice NetCDF files,\n",
    "following Google Python Style Guide and Amazon engineering best practices.\n",
    "\n",
    "Author: Xinlong Liu\n",
    "Date: December 2025\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "\n",
    "# Configure logging following Google/Amazon standards\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VariableInfo:\n",
    "    \"\"\"Data class to store variable metadata following Google style.\"\"\"\n",
    "    name: str\n",
    "    dtype: str\n",
    "    shape: Tuple[int, ...]\n",
    "    dimensions: Tuple[str, ...]\n",
    "    units: Optional[str]\n",
    "    long_name: Optional[str]\n",
    "    valid_count: int\n",
    "    nan_count: int\n",
    "    fill_value: Optional[Any]\n",
    "\n",
    "\n",
    "class NetCDFInspector:\n",
    "    \"\"\"\n",
    "    A professional-grade NetCDF file inspector for Cryo-TEMPO products.\n",
    "    \n",
    "    This class provides comprehensive inspection capabilities following\n",
    "    enterprise software engineering standards used at Google and Amazon.\n",
    "    \n",
    "    Attributes:\n",
    "        file_path: Path to the NetCDF file to inspect.\n",
    "        dataset: xarray Dataset object containing the loaded data.\n",
    "        \n",
    "    Example:\n",
    "        >>> inspector = NetCDFInspector(file_path)\n",
    "        >>> inspector.run_full_inspection()\n",
    "    \"\"\"\n",
    "    \n",
    "    SEPARATOR = \"=\" * 80\n",
    "    SUB_SEPARATOR = \"-\" * 60\n",
    "    \n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the NetCDF inspector.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Absolute path to the NetCDF file.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the specified file does not exist.\n",
    "            ValueError: If the file is not a valid NetCDF file.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self._validate_file_path()\n",
    "        self.dataset: Optional[xr.Dataset] = None\n",
    "        self.nc_dataset: Optional[nc.Dataset] = None\n",
    "        \n",
    "    def _validate_file_path(self) -> None:\n",
    "        \"\"\"Validate that the file exists and has correct extension.\"\"\"\n",
    "        if not os.path.exists(self.file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {self.file_path}\")\n",
    "        if not self.file_path.endswith('.nc'):\n",
    "            raise ValueError(f\"Expected NetCDF file (.nc), got: {self.file_path}\")\n",
    "        logger.info(f\"File path validated: {self.file_path}\")\n",
    "    \n",
    "    def load_dataset(self) -> None:\n",
    "        \"\"\"Load the NetCDF dataset using both xarray and netCDF4.\"\"\"\n",
    "        logger.info(\"Loading NetCDF dataset...\")\n",
    "        try:\n",
    "            self.dataset = xr.open_dataset(self.file_path)\n",
    "            self.nc_dataset = nc.Dataset(self.file_path, 'r')\n",
    "            logger.info(\"Dataset loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load dataset: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def display_file_info(self) -> None:\n",
    "        \"\"\"Display basic file information.\"\"\"\n",
    "        print(f\"\\n{self.SEPARATOR}\")\n",
    "        print(\"1. FILE INFORMATION\")\n",
    "        print(self.SEPARATOR)\n",
    "        \n",
    "        file_stats = os.stat(self.file_path)\n",
    "        file_size_mb = file_stats.st_size / (1024 * 1024)\n",
    "        \n",
    "        info_table = [\n",
    "            [\"File Name\", os.path.basename(self.file_path)],\n",
    "            [\"File Path\", self.file_path],\n",
    "            [\"File Size\", f\"{file_size_mb:.4f} MB ({file_stats.st_size:,} bytes)\"],\n",
    "            [\"Last Modified\", datetime.fromtimestamp(file_stats.st_mtime).strftime('%Y-%m-%d %H:%M:%S')],\n",
    "            [\"NetCDF Format\", self.nc_dataset.file_format if self.nc_dataset else \"Unknown\"],\n",
    "        ]\n",
    "        \n",
    "        df_info = pd.DataFrame(info_table, columns=[\"Property\", \"Value\"])\n",
    "        print(df_info.to_string(index=False))\n",
    "    \n",
    "    def display_global_attributes(self) -> None:\n",
    "        \"\"\"Display all global attributes (metadata) of the NetCDF file.\"\"\"\n",
    "        print(f\"\\n{self.SEPARATOR}\")\n",
    "        print(\"2. GLOBAL ATTRIBUTES (METADATA)\")\n",
    "        print(self.SEPARATOR)\n",
    "        \n",
    "        if self.nc_dataset is None:\n",
    "            logger.warning(\"Dataset not loaded. Call load_dataset() first.\")\n",
    "            return\n",
    "        \n",
    "        global_attrs = self.nc_dataset.ncattrs()\n",
    "        \n",
    "        if not global_attrs:\n",
    "            print(\"No global attributes found.\")\n",
    "            return\n",
    "        \n",
    "        attr_data = []\n",
    "        for attr in global_attrs:\n",
    "            value = self.nc_dataset.getncattr(attr)\n",
    "            # Truncate long values for display\n",
    "            value_str = str(value)\n",
    "            if len(value_str) > 80:\n",
    "                value_str = value_str[:77] + \"...\"\n",
    "            attr_data.append([attr, value_str])\n",
    "        \n",
    "        df_attrs = pd.DataFrame(attr_data, columns=[\"Attribute\", \"Value\"])\n",
    "        print(df_attrs.to_string(index=False))\n",
    "        print(f\"\\nTotal Global Attributes: {len(global_attrs)}\")\n",
    "    \n",
    "    def display_dimensions(self) -> None:\n",
    "        \"\"\"Display all dimensions in the NetCDF file.\"\"\"\n",
    "        print(f\"\\n{self.SEPARATOR}\")\n",
    "        print(\"3. DIMENSIONS\")\n",
    "        print(self.SEPARATOR)\n",
    "        \n",
    "        if self.nc_dataset is None:\n",
    "            logger.warning(\"Dataset not loaded. Call load_dataset() first.\")\n",
    "            return\n",
    "        \n",
    "        dim_data = []\n",
    "        for dim_name, dim in self.nc_dataset.dimensions.items():\n",
    "            dim_data.append([\n",
    "                dim_name,\n",
    "                len(dim),\n",
    "                \"Unlimited\" if dim.isunlimited() else \"Fixed\"\n",
    "            ])\n",
    "        \n",
    "        df_dims = pd.DataFrame(dim_data, columns=[\"Dimension\", \"Size\", \"Type\"])\n",
    "        print(df_dims.to_string(index=False))\n",
    "        print(f\"\\nTotal Dimensions: {len(self.nc_dataset.dimensions)}\")\n",
    "    \n",
    "    def display_variables(self) -> None:\n",
    "        \"\"\"Display comprehensive information about all variables.\"\"\"\n",
    "        print(f\"\\n{self.SEPARATOR}\")\n",
    "        print(\"4. VARIABLES\")\n",
    "        print(self.SEPARATOR)\n",
    "        \n",
    "        if self.dataset is None:\n",
    "            logger.warning(\"Dataset not loaded. Call load_dataset() first.\")\n",
    "            return\n",
    "        \n",
    "        var_data = []\n",
    "        for var_name in self.dataset.data_vars:\n",
    "            var = self.dataset[var_name]\n",
    "            \n",
    "            # Extract attributes safely\n",
    "            units = var.attrs.get('units', 'N/A')\n",
    "            long_name = var.attrs.get('long_name', 'N/A')\n",
    "            \n",
    "            # Calculate valid/NaN counts\n",
    "            try:\n",
    "                values = var.values\n",
    "                if np.issubdtype(values.dtype, np.floating):\n",
    "                    nan_count = int(np.sum(np.isnan(values)))\n",
    "                    valid_count = int(np.sum(~np.isnan(values)))\n",
    "                else:\n",
    "                    nan_count = 0\n",
    "                    valid_count = int(values.size)\n",
    "            except Exception:\n",
    "                nan_count = \"N/A\"\n",
    "                valid_count = \"N/A\"\n",
    "            \n",
    "            var_data.append([\n",
    "                var_name,\n",
    "                str(var.dtype),\n",
    "                str(var.shape),\n",
    "                str(var.dims),\n",
    "                units,\n",
    "                valid_count,\n",
    "                nan_count\n",
    "            ])\n",
    "        \n",
    "        df_vars = pd.DataFrame(\n",
    "            var_data,\n",
    "            columns=[\"Variable\", \"DType\", \"Shape\", \"Dimensions\", \"Units\", \"Valid\", \"NaN\"]\n",
    "        )\n",
    "        print(df_vars.to_string(index=False))\n",
    "        print(f\"\\nTotal Variables: {len(self.dataset.data_vars)}\")\n",
    "        \n",
    "        # Also display coordinate variables\n",
    "        print(f\"\\n{self.SUB_SEPARATOR}\")\n",
    "        print(\"Coordinate Variables:\")\n",
    "        print(self.SUB_SEPARATOR)\n",
    "        \n",
    "        coord_data = []\n",
    "        for coord_name in self.dataset.coords:\n",
    "            coord = self.dataset.coords[coord_name]\n",
    "            coord_data.append([\n",
    "                coord_name,\n",
    "                str(coord.dtype),\n",
    "                str(coord.shape),\n",
    "                coord.attrs.get('units', 'N/A')\n",
    "            ])\n",
    "        \n",
    "        df_coords = pd.DataFrame(\n",
    "            coord_data,\n",
    "            columns=[\"Coordinate\", \"DType\", \"Shape\", \"Units\"]\n",
    "        )\n",
    "        print(df_coords.to_string(index=False))\n",
    "    \n",
    "    def display_variable_details(self) -> None:\n",
    "        \"\"\"Display detailed attributes for each variable.\"\"\"\n",
    "        print(f\"\\n{self.SEPARATOR}\")\n",
    "        print(\"5. VARIABLE DETAILED ATTRIBUTES\")\n",
    "        print(self.SEPARATOR)\n",
    "        \n",
    "        if self.dataset is None:\n",
    "            logger.warning(\"Dataset not loaded. Call load_dataset() first.\")\n",
    "            return\n",
    "        \n",
    "        for var_name in self.dataset.data_vars:\n",
    "            var = self.dataset[var_name]\n",
    "            print(f\"\\n{self.SUB_SEPARATOR}\")\n",
    "            print(f\"Variable: {var_name}\")\n",
    "            print(self.SUB_SEPARATOR)\n",
    "            \n",
    "            if var.attrs:\n",
    "                for attr_name, attr_value in var.attrs.items():\n",
    "                    value_str = str(attr_value)\n",
    "                    if len(value_str) > 70:\n",
    "                        value_str = value_str[:67] + \"...\"\n",
    "                    print(f\"  {attr_name}: {value_str}\")\n",
    "            else:\n",
    "                print(\"  No attributes found.\")\n",
    "    \n",
    "    def display_statistics(self) -> None:\n",
    "        \"\"\"Display summary statistics for numeric variables.\"\"\"\n",
    "        print(f\"\\n{self.SEPARATOR}\")\n",
    "        print(\"6. SUMMARY STATISTICS (Numeric Variables)\")\n",
    "        print(self.SEPARATOR)\n",
    "        \n",
    "        if self.dataset is None:\n",
    "            logger.warning(\"Dataset not loaded. Call load_dataset() first.\")\n",
    "            return\n",
    "        \n",
    "        stats_data = []\n",
    "        for var_name in self.dataset.data_vars:\n",
    "            var = self.dataset[var_name]\n",
    "            \n",
    "            # Only process numeric types\n",
    "            if not np.issubdtype(var.dtype, np.number):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                values = var.values.flatten()\n",
    "                # Handle NaN values\n",
    "                valid_values = values[~np.isnan(values)] if np.issubdtype(values.dtype, np.floating) else values\n",
    "                \n",
    "                if len(valid_values) == 0:\n",
    "                    continue\n",
    "                \n",
    "                stats_data.append([\n",
    "                    var_name,\n",
    "                    f\"{np.min(valid_values):.6g}\",\n",
    "                    f\"{np.max(valid_values):.6g}\",\n",
    "                    f\"{np.mean(valid_values):.6g}\",\n",
    "                    f\"{np.median(valid_values):.6g}\",\n",
    "                    f\"{np.std(valid_values):.6g}\",\n",
    "                    len(valid_values)\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Could not compute stats for {var_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if stats_data:\n",
    "            df_stats = pd.DataFrame(\n",
    "                stats_data,\n",
    "                columns=[\"Variable\", \"Min\", \"Max\", \"Mean\", \"Median\", \"Std Dev\", \"Count\"]\n",
    "            )\n",
    "            print(df_stats.to_string(index=False))\n",
    "        else:\n",
    "            print(\"No numeric variables found for statistics.\")\n",
    "    \n",
    "    def display_data_quality_summary(self) -> None:\n",
    "        \"\"\"Display data quality indicators and potential issues.\"\"\"\n",
    "        print(f\"\\n{self.SEPARATOR}\")\n",
    "        print(\"7. DATA QUALITY SUMMARY\")\n",
    "        print(self.SEPARATOR)\n",
    "        \n",
    "        if self.dataset is None:\n",
    "            logger.warning(\"Dataset not loaded. Call load_dataset() first.\")\n",
    "            return\n",
    "        \n",
    "        issues = []\n",
    "        quality_data = []\n",
    "        \n",
    "        for var_name in self.dataset.data_vars:\n",
    "            var = self.dataset[var_name]\n",
    "            \n",
    "            try:\n",
    "                values = var.values.flatten()\n",
    "                total = len(values)\n",
    "                \n",
    "                if np.issubdtype(values.dtype, np.floating):\n",
    "                    nan_count = int(np.sum(np.isnan(values)))\n",
    "                    nan_pct = (nan_count / total) * 100 if total > 0 else 0\n",
    "                    \n",
    "                    quality_data.append([\n",
    "                        var_name,\n",
    "                        total,\n",
    "                        total - nan_count,\n",
    "                        nan_count,\n",
    "                        f\"{nan_pct:.2f}%\"\n",
    "                    ])\n",
    "                    \n",
    "                    if nan_pct > 50:\n",
    "                        issues.append(f\"WARNING: {var_name} has {nan_pct:.1f}% missing values\")\n",
    "                        \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if quality_data:\n",
    "            df_quality = pd.DataFrame(\n",
    "                quality_data,\n",
    "                columns=[\"Variable\", \"Total\", \"Valid\", \"Missing\", \"Missing %\"]\n",
    "            )\n",
    "            print(df_quality.to_string(index=False))\n",
    "        \n",
    "        if issues:\n",
    "            print(f\"\\n{self.SUB_SEPARATOR}\")\n",
    "            print(\"Data Quality Issues Detected:\")\n",
    "            print(self.SUB_SEPARATOR)\n",
    "            for issue in issues:\n",
    "                print(f\"  ⚠ {issue}\")\n",
    "        else:\n",
    "            print(\"\\n✓ No significant data quality issues detected.\")\n",
    "    \n",
    "    def run_full_inspection(self) -> None:\n",
    "        \"\"\"Execute the complete inspection workflow.\"\"\"\n",
    "        logger.info(\"Starting full NetCDF inspection...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"  CRYO-TEMPO NETCDF DATASET INSPECTION REPORT\")\n",
    "        print(\"  Generated: \" + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        self.load_dataset()\n",
    "        self.display_file_info()\n",
    "        self.display_global_attributes()\n",
    "        self.display_dimensions()\n",
    "        self.display_variables()\n",
    "        self.display_variable_details()\n",
    "        self.display_statistics()\n",
    "        self.display_data_quality_summary()\n",
    "        \n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"\\n{self.SEPARATOR}\")\n",
    "        print(f\"Inspection completed in {elapsed:.2f} seconds.\")\n",
    "        print(self.SEPARATOR)\n",
    "        \n",
    "        logger.info(f\"Inspection completed in {elapsed:.2f} seconds.\")\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close all open dataset handles.\"\"\"\n",
    "        if self.dataset is not None:\n",
    "            self.dataset.close()\n",
    "        if self.nc_dataset is not None:\n",
    "            self.nc_dataset.close()\n",
    "        logger.info(\"Dataset handles closed.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\" or True:  # Always execute in notebook context\n",
    "    \n",
    "    # Define file path for the first Cryo-TEMPO observation in July 2010\n",
    "    FILE_PATH = r\"D:\\phd\\data\\CryoTEMPO\\2010\\07\\CS_OFFL_SIR_TDP_SI_ANTARC_20100716T000456_20100716T000635_02_03041_C001.nc\"\n",
    "    \n",
    "    # Create inspector instance and run full inspection\n",
    "    try:\n",
    "        inspector = NetCDFInspector(FILE_PATH)\n",
    "        inspector.run_full_inspection()\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"File not found: {e}\")\n",
    "        print(f\"\\n❌ ERROR: {e}\")\n",
    "        print(\"Please verify the file path and ensure the file exists.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        print(f\"\\n❌ ERROR: {e}\")\n",
    "    finally:\n",
    "        # Ensure resources are cleaned up\n",
    "        if 'inspector' in dir() and inspector is not None:\n",
    "            inspector.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a500286",
   "metadata": {},
   "source": [
    "# Cryo-TEMPO Variable Validation and Extraction Pipeline\n",
    "\n",
    "## Overview\n",
    "This pipeline validates the existence of critical sea ice freeboard variables across the **entire Cryo-TEMPO dataset archive** (July 2010 - May 2024) and consolidates them into a single NetCDF file for downstream analysis.\n",
    "\n",
    "### Target Variables\n",
    "| Variable | Description | Unit |\n",
    "|----------|-------------|------|\n",
    "| `radar_freeboard` | Radar freeboard height | m |\n",
    "| `radar_freeboard_uncertainty` | Uncertainty of radar freeboard | m |\n",
    "| `sea_ice_freeboard` | Sea ice freeboard height | m |\n",
    "| `sea_ice_freeboard_uncertainty` | Uncertainty of sea ice freeboard | m |\n",
    "| `snow_depth` | Snow depth on sea ice | m |\n",
    "| `snow_depth_uncertainty` | Uncertainty of snow depth | m |\n",
    "\n",
    "### Data Source\n",
    "- **Directory**: `D:\\phd\\data\\CryoTEMPO`\n",
    "- **Time Period**: July 2010 to May 2024\n",
    "- **Structure**: `{base_dir}/{year}/{month}/{files}.nc`\n",
    "\n",
    "### Pipeline Stages\n",
    "1. **Discovery**: Scan directory structure to identify all NetCDF files\n",
    "2. **Validation**: Verify target variables exist in all files\n",
    "3. **Extraction**: Extract and consolidate variables from all files\n",
    "4. **Export**: Save consolidated dataset with CF-compliant metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1f2ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "  CRYO-TEMPO VARIABLE VALIDATION AND EXTRACTION PIPELINE\n",
      "  Configuration Loaded Successfully\n",
      "================================================================================\n",
      "\n",
      "  Base Directory    : D:\\phd\\data\\CryoTEMPO\n",
      "  Time Period       : 2010/07 - 2024/05\n",
      "  Target Variables  : 6\n",
      "  Max Workers       : 8\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cryo-TEMPO Variable Validation and Extraction Pipeline\n",
    "\n",
    "This module provides enterprise-grade data validation and extraction capabilities\n",
    "for Cryo-TEMPO sea ice freeboard products, following Google Python Style Guide\n",
    "and Amazon engineering best practices.\n",
    "\n",
    "Author: Xinlong Liu\n",
    "Date: December 2025\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import logging\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Set, Any\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import defaultdict\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(\"CryoTEMPO.Pipeline\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PipelineConfig:\n",
    "    \"\"\"Immutable configuration for the Cryo-TEMPO extraction pipeline.\"\"\"\n",
    "    \n",
    "    # Base directory containing all Cryo-TEMPO data\n",
    "    BASE_DIR: str = r\"D:\\phd\\data\\CryoTEMPO\"\n",
    "    \n",
    "    # Output directory for consolidated dataset\n",
    "    OUTPUT_DIR: str = r\"D:\\phd\\data\\CryoTEMPO\"\n",
    "    \n",
    "    # Time period boundaries\n",
    "    START_YEAR: int = 2010\n",
    "    START_MONTH: int = 7  # July\n",
    "    END_YEAR: int = 2024\n",
    "    END_MONTH: int = 5    # May\n",
    "    \n",
    "    # Target variables to validate and extract\n",
    "    TARGET_VARIABLES: Tuple[str, ...] = (\n",
    "        \"radar_freeboard\",\n",
    "        \"radar_freeboard_uncertainty\",\n",
    "        \"sea_ice_freeboard\",\n",
    "        \"sea_ice_freeboard_uncertainty\",\n",
    "        \"snow_depth\",\n",
    "        \"snow_depth_uncertainty\",\n",
    "    )\n",
    "    \n",
    "    # Additional variables to extract for context\n",
    "    CONTEXT_VARIABLES: Tuple[str, ...] = (\n",
    "        \"time\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "    )\n",
    "    \n",
    "    # File pattern for Cryo-TEMPO files\n",
    "    FILE_PATTERN: str = \"CS_OFFL_SIR_TDP_SI_ANTARC_*.nc\"\n",
    "    \n",
    "    # Maximum workers for parallel processing\n",
    "    MAX_WORKERS: int = 8\n",
    "    \n",
    "    # Chunk size for memory-efficient processing\n",
    "    CHUNK_SIZE: int = 100\n",
    "\n",
    "\n",
    "# Initialize configuration\n",
    "CONFIG = PipelineConfig()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA CLASSES FOR STRUCTURED RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FileValidationResult:\n",
    "    \"\"\"Result of validating a single NetCDF file.\"\"\"\n",
    "    file_path: str\n",
    "    is_valid: bool\n",
    "    missing_variables: List[str] = field(default_factory=list)\n",
    "    error_message: Optional[str] = None\n",
    "    record_count: int = 0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MonthlyValidationSummary:\n",
    "    \"\"\"Summary of validation results for a single month.\"\"\"\n",
    "    year: int\n",
    "    month: int\n",
    "    total_files: int\n",
    "    valid_files: int\n",
    "    invalid_files: int\n",
    "    missing_variable_counts: Dict[str, int] = field(default_factory=dict)\n",
    "    errors: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineReport:\n",
    "    \"\"\"Comprehensive report of the entire pipeline execution.\"\"\"\n",
    "    start_time: datetime\n",
    "    end_time: Optional[datetime] = None\n",
    "    total_files_scanned: int = 0\n",
    "    total_files_valid: int = 0\n",
    "    total_files_invalid: int = 0\n",
    "    total_records_extracted: int = 0\n",
    "    monthly_summaries: List[MonthlyValidationSummary] = field(default_factory=list)\n",
    "    output_file_path: Optional[str] = None\n",
    "    success: bool = False\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"  CRYO-TEMPO VARIABLE VALIDATION AND EXTRACTION PIPELINE\")\n",
    "print(\"  Configuration Loaded Successfully\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n  Base Directory    : {CONFIG.BASE_DIR}\")\n",
    "print(f\"  Time Period       : {CONFIG.START_YEAR}/{CONFIG.START_MONTH:02d} - {CONFIG.END_YEAR}/{CONFIG.END_MONTH:02d}\")\n",
    "print(f\"  Target Variables  : {len(CONFIG.TARGET_VARIABLES)}\")\n",
    "print(f\"  Max Workers       : {CONFIG.MAX_WORKERS}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ceafe8",
   "metadata": {},
   "source": [
    "## Stage 1: File Discovery\n",
    "\n",
    "This stage scans the directory structure to identify all NetCDF files within the specified time period. The discovery process:\n",
    "- Iterates through each year-month combination\n",
    "- Validates directory existence\n",
    "- Counts files per month for initial assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc465067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 20:51:52 - INFO - CryoTEMPO.Pipeline - Starting file discovery process...\n",
      "2025-12-07 20:51:52 - INFO - CryoTEMPO.Pipeline - Generated 167 time periods to scan.\n",
      "2025-12-07 20:54:25 - INFO - CryoTEMPO.Pipeline - Discovery completed: 179876 files found in 153.02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  FILE DISCOVERY SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Files per Year:\n",
      "----------------------------------------\n",
      "  2010: 5,312 files\n",
      "  2011: 13,957 files\n",
      "  2012: 13,388 files\n",
      "  2013: 12,664 files\n",
      "  2014: 12,964 files\n",
      "  2015: 12,787 files\n",
      "  2016: 12,817 files\n",
      "  2017: 12,682 files\n",
      "  2018: 12,791 files\n",
      "  2019: 12,862 files\n",
      "  2020: 13,089 files\n",
      "  2021: 12,870 files\n",
      "  2022: 13,005 files\n",
      "  2023: 13,125 files\n",
      "  2024: 5,563 files\n",
      "\n",
      "========================================\n",
      "  TOTAL FILES: 179,876\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 1: File Discovery\n",
    "Scan the directory structure to identify all Cryo-TEMPO NetCDF files.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class FileDiscoveryEngine:\n",
    "    \"\"\"\n",
    "    Engine for discovering Cryo-TEMPO NetCDF files across the archive.\n",
    "    \n",
    "    This class implements efficient directory scanning with comprehensive\n",
    "    logging and error handling following enterprise standards.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the file discovery engine.\n",
    "        \n",
    "        Args:\n",
    "            config: Pipeline configuration object.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.file_inventory: Dict[Tuple[int, int], List[str]] = {}\n",
    "        \n",
    "    def generate_time_periods(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Generate list of (year, month) tuples for the target time period.\n",
    "        \n",
    "        Returns:\n",
    "            List of (year, month) tuples from start to end period.\n",
    "        \"\"\"\n",
    "        periods = []\n",
    "        \n",
    "        for year in range(self.config.START_YEAR, self.config.END_YEAR + 1):\n",
    "            start_month = self.config.START_MONTH if year == self.config.START_YEAR else 1\n",
    "            end_month = self.config.END_MONTH if year == self.config.END_YEAR else 12\n",
    "            \n",
    "            for month in range(start_month, end_month + 1):\n",
    "                periods.append((year, month))\n",
    "        \n",
    "        logger.info(f\"Generated {len(periods)} time periods to scan.\")\n",
    "        return periods\n",
    "    \n",
    "    def discover_files_for_period(self, year: int, month: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Discover all NetCDF files for a specific year-month.\n",
    "        \n",
    "        Args:\n",
    "            year: Target year.\n",
    "            month: Target month (1-12).\n",
    "            \n",
    "        Returns:\n",
    "            List of absolute file paths found.\n",
    "        \"\"\"\n",
    "        month_dir = os.path.join(\n",
    "            self.config.BASE_DIR, \n",
    "            str(year), \n",
    "            f\"{month:02d}\"\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(month_dir):\n",
    "            logger.warning(f\"Directory not found: {month_dir}\")\n",
    "            return []\n",
    "        \n",
    "        pattern = os.path.join(month_dir, self.config.FILE_PATTERN)\n",
    "        files = glob.glob(pattern)\n",
    "        \n",
    "        return sorted(files)\n",
    "    \n",
    "    def run_discovery(self) -> Dict[Tuple[int, int], List[str]]:\n",
    "        \"\"\"\n",
    "        Execute the full file discovery process.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping (year, month) to list of file paths.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting file discovery process...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        periods = self.generate_time_periods()\n",
    "        total_files = 0\n",
    "        \n",
    "        for year, month in periods:\n",
    "            files = self.discover_files_for_period(year, month)\n",
    "            self.file_inventory[(year, month)] = files\n",
    "            total_files += len(files)\n",
    "        \n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        logger.info(f\"Discovery completed: {total_files} files found in {elapsed:.2f}s\")\n",
    "        \n",
    "        return self.file_inventory\n",
    "    \n",
    "    def print_discovery_summary(self) -> None:\n",
    "        \"\"\"Print a formatted summary of discovered files.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"  FILE DISCOVERY SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        summary_data = []\n",
    "        total_files = 0\n",
    "        \n",
    "        for (year, month), files in sorted(self.file_inventory.items()):\n",
    "            summary_data.append([year, month, len(files)])\n",
    "            total_files += len(files)\n",
    "        \n",
    "        # Create yearly summary\n",
    "        yearly_counts = defaultdict(int)\n",
    "        for (year, month), files in self.file_inventory.items():\n",
    "            yearly_counts[year] += len(files)\n",
    "        \n",
    "        print(\"\\nFiles per Year:\")\n",
    "        print(\"-\" * 40)\n",
    "        for year in sorted(yearly_counts.keys()):\n",
    "            print(f\"  {year}: {yearly_counts[year]:,} files\")\n",
    "        \n",
    "        print(f\"\\n{'=' * 40}\")\n",
    "        print(f\"  TOTAL FILES: {total_files:,}\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "\n",
    "# Execute file discovery\n",
    "discovery_engine = FileDiscoveryEngine(CONFIG)\n",
    "file_inventory = discovery_engine.run_discovery()\n",
    "discovery_engine.print_discovery_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af4739",
   "metadata": {},
   "source": [
    "## Stage 2: Variable Validation\n",
    "\n",
    "This stage validates that all target variables exist in every NetCDF file. The validation process:\n",
    "- Opens each file and checks for required variables\n",
    "- Tracks missing variables per file\n",
    "- Generates comprehensive validation reports\n",
    "- Identifies any data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf602bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 20:56:03 - INFO - CryoTEMPO.Pipeline - Starting variable validation process...\n",
      "2025-12-07 20:58:24 - INFO - CryoTEMPO.Pipeline - Validated 2010: 5312/179876 files processed\n",
      "2025-12-07 21:08:00 - INFO - CryoTEMPO.Pipeline - Validated 2011: 19269/179876 files processed\n",
      "2025-12-07 21:17:26 - INFO - CryoTEMPO.Pipeline - Validated 2012: 32657/179876 files processed\n",
      "2025-12-07 21:23:25 - INFO - CryoTEMPO.Pipeline - Validated 2013: 45321/179876 files processed\n",
      "2025-12-07 21:30:58 - INFO - CryoTEMPO.Pipeline - Validated 2014: 58285/179876 files processed\n",
      "2025-12-07 21:40:49 - INFO - CryoTEMPO.Pipeline - Validated 2015: 71072/179876 files processed\n",
      "2025-12-07 21:52:47 - INFO - CryoTEMPO.Pipeline - Validated 2016: 83889/179876 files processed\n",
      "2025-12-07 22:01:33 - INFO - CryoTEMPO.Pipeline - Validated 2017: 96571/179876 files processed\n",
      "2025-12-07 22:08:26 - INFO - CryoTEMPO.Pipeline - Validated 2018: 109362/179876 files processed\n",
      "2025-12-07 22:15:26 - INFO - CryoTEMPO.Pipeline - Validated 2019: 122224/179876 files processed\n",
      "2025-12-07 22:22:19 - INFO - CryoTEMPO.Pipeline - Validated 2020: 135313/179876 files processed\n",
      "2025-12-07 22:28:42 - INFO - CryoTEMPO.Pipeline - Validated 2021: 148183/179876 files processed\n",
      "2025-12-07 22:34:51 - INFO - CryoTEMPO.Pipeline - Validated 2022: 161188/179876 files processed\n",
      "2025-12-07 22:41:45 - INFO - CryoTEMPO.Pipeline - Validated 2023: 174313/179876 files processed\n",
      "2025-12-07 22:44:29 - INFO - CryoTEMPO.Pipeline - Validated 2024: 179876/179876 files processed\n",
      "2025-12-07 22:44:29 - INFO - CryoTEMPO.Pipeline - Validation completed in 6505.84s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  VARIABLE VALIDATION REPORT\n",
      "================================================================================\n",
      "\n",
      "Target Variables: radar_freeboard, radar_freeboard_uncertainty, sea_ice_freeboard, sea_ice_freeboard_uncertainty, snow_depth, snow_depth_uncertainty\n",
      "\n",
      "============================================================\n",
      "  Total Files Scanned : 179,876\n",
      "  Valid Files         : 179,872 (100.00%)\n",
      "  Invalid Files       : 4 (0.00%)\n",
      "============================================================\n",
      "\n",
      "⚠ Errors Detected (4 total):\n",
      "----------------------------------------\n",
      "  CS_OFFL_SIR_TDP_SI_ANTARC_20101128T231145_20101128T231416_02_05014_C001.nc: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'scipy']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html\n",
      "  CS_OFFL_SIR_TDP_SI_ANTARC_20101221T084100_20101221T084146_02_05339_C001.nc: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'scipy']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html\n",
      "  CS_OFFL_SIR_TDP_SI_ANTARC_20110226T143225_20110226T143344_03_00971_C001.nc: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'scipy']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html\n",
      "  CS_OFFL_SIR_TDP_SI_ANTARC_20110226T143448_20110226T143510_03_00971_C001.nc: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'scipy']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html\n",
      "\n",
      "============================================================\n",
      "  ⚠ VALIDATION COMPLETED WITH ISSUES\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 2: Variable Validation\n",
    "Validate that all target variables exist in every NetCDF file.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class VariableValidator:\n",
    "    \"\"\"\n",
    "    Validator for checking variable existence across NetCDF files.\n",
    "    \n",
    "    This class implements comprehensive validation with detailed\n",
    "    reporting following enterprise data quality standards.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the variable validator.\n",
    "        \n",
    "        Args:\n",
    "            config: Pipeline configuration object.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.validation_results: List[FileValidationResult] = []\n",
    "        \n",
    "    def validate_single_file(self, file_path: str) -> FileValidationResult:\n",
    "        \"\"\"\n",
    "        Validate a single NetCDF file for required variables.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the NetCDF file.\n",
    "            \n",
    "        Returns:\n",
    "            FileValidationResult with validation outcome.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with xr.open_dataset(file_path) as ds:\n",
    "                available_vars = set(ds.data_vars.keys()) | set(ds.coords.keys())\n",
    "                required_vars = set(self.config.TARGET_VARIABLES) | set(self.config.CONTEXT_VARIABLES)\n",
    "                \n",
    "                missing = list(required_vars - available_vars)\n",
    "                record_count = ds.dims.get('time', len(ds.time) if 'time' in ds.coords else 0)\n",
    "                \n",
    "                return FileValidationResult(\n",
    "                    file_path=file_path,\n",
    "                    is_valid=(len(missing) == 0),\n",
    "                    missing_variables=missing,\n",
    "                    record_count=record_count\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            return FileValidationResult(\n",
    "                file_path=file_path,\n",
    "                is_valid=False,\n",
    "                error_message=str(e)\n",
    "            )\n",
    "    \n",
    "    def validate_monthly_files(\n",
    "        self, \n",
    "        year: int, \n",
    "        month: int, \n",
    "        files: List[str]\n",
    "    ) -> MonthlyValidationSummary:\n",
    "        \"\"\"\n",
    "        Validate all files for a specific month.\n",
    "        \n",
    "        Args:\n",
    "            year: Target year.\n",
    "            month: Target month.\n",
    "            files: List of file paths to validate.\n",
    "            \n",
    "        Returns:\n",
    "            MonthlyValidationSummary with aggregated results.\n",
    "        \"\"\"\n",
    "        valid_count = 0\n",
    "        invalid_count = 0\n",
    "        missing_var_counts: Dict[str, int] = defaultdict(int)\n",
    "        errors: List[str] = []\n",
    "        \n",
    "        for file_path in files:\n",
    "            result = self.validate_single_file(file_path)\n",
    "            self.validation_results.append(result)\n",
    "            \n",
    "            if result.is_valid:\n",
    "                valid_count += 1\n",
    "            else:\n",
    "                invalid_count += 1\n",
    "                for var in result.missing_variables:\n",
    "                    missing_var_counts[var] += 1\n",
    "                if result.error_message:\n",
    "                    errors.append(f\"{os.path.basename(file_path)}: {result.error_message}\")\n",
    "        \n",
    "        return MonthlyValidationSummary(\n",
    "            year=year,\n",
    "            month=month,\n",
    "            total_files=len(files),\n",
    "            valid_files=valid_count,\n",
    "            invalid_files=invalid_count,\n",
    "            missing_variable_counts=dict(missing_var_counts),\n",
    "            errors=errors\n",
    "        )\n",
    "    \n",
    "    def run_validation(\n",
    "        self, \n",
    "        file_inventory: Dict[Tuple[int, int], List[str]]\n",
    "    ) -> List[MonthlyValidationSummary]:\n",
    "        \"\"\"\n",
    "        Execute validation across all discovered files.\n",
    "        \n",
    "        Args:\n",
    "            file_inventory: Dictionary mapping (year, month) to file lists.\n",
    "            \n",
    "        Returns:\n",
    "            List of MonthlyValidationSummary objects.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting variable validation process...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        summaries = []\n",
    "        total_files = sum(len(files) for files in file_inventory.values())\n",
    "        processed = 0\n",
    "        \n",
    "        for (year, month), files in sorted(file_inventory.items()):\n",
    "            if not files:\n",
    "                continue\n",
    "                \n",
    "            summary = self.validate_monthly_files(year, month, files)\n",
    "            summaries.append(summary)\n",
    "            processed += len(files)\n",
    "            \n",
    "            # Progress update every year\n",
    "            if month == 12 or (year == self.config.END_YEAR and month == self.config.END_MONTH):\n",
    "                logger.info(f\"Validated {year}: {processed}/{total_files} files processed\")\n",
    "        \n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        logger.info(f\"Validation completed in {elapsed:.2f}s\")\n",
    "        \n",
    "        return summaries\n",
    "    \n",
    "    def print_validation_report(self, summaries: List[MonthlyValidationSummary]) -> bool:\n",
    "        \"\"\"\n",
    "        Print comprehensive validation report.\n",
    "        \n",
    "        Args:\n",
    "            summaries: List of monthly validation summaries.\n",
    "            \n",
    "        Returns:\n",
    "            True if all files are valid, False otherwise.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"  VARIABLE VALIDATION REPORT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        total_files = sum(s.total_files for s in summaries)\n",
    "        total_valid = sum(s.valid_files for s in summaries)\n",
    "        total_invalid = sum(s.invalid_files for s in summaries)\n",
    "        \n",
    "        print(f\"\\nTarget Variables: {', '.join(self.config.TARGET_VARIABLES)}\")\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"  Total Files Scanned : {total_files:,}\")\n",
    "        print(f\"  Valid Files         : {total_valid:,} ({100*total_valid/total_files:.2f}%)\")\n",
    "        print(f\"  Invalid Files       : {total_invalid:,} ({100*total_invalid/total_files:.2f}%)\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        \n",
    "        # Check for any missing variables\n",
    "        all_missing = defaultdict(int)\n",
    "        for s in summaries:\n",
    "            for var, count in s.missing_variable_counts.items():\n",
    "                all_missing[var] += count\n",
    "        \n",
    "        if all_missing:\n",
    "            print(\"\\n⚠ Missing Variables Detected:\")\n",
    "            print(\"-\" * 40)\n",
    "            for var, count in sorted(all_missing.items(), key=lambda x: -x[1]):\n",
    "                print(f\"  {var}: missing in {count} files\")\n",
    "        \n",
    "        # Collect all errors\n",
    "        all_errors = []\n",
    "        for s in summaries:\n",
    "            all_errors.extend(s.errors)\n",
    "        \n",
    "        if all_errors:\n",
    "            print(f\"\\n⚠ Errors Detected ({len(all_errors)} total):\")\n",
    "            print(\"-\" * 40)\n",
    "            for error in all_errors[:10]:  # Show first 10\n",
    "                print(f\"  {error}\")\n",
    "            if len(all_errors) > 10:\n",
    "                print(f\"  ... and {len(all_errors) - 10} more errors\")\n",
    "        \n",
    "        all_valid = (total_invalid == 0)\n",
    "        \n",
    "        if all_valid:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"  ✓ ALL FILES VALIDATED SUCCESSFULLY\")\n",
    "            print(\"  All target variables exist in all datasets.\")\n",
    "            print(\"=\" * 60)\n",
    "        else:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"  ⚠ VALIDATION COMPLETED WITH ISSUES\")\n",
    "            print(\"=\" * 60)\n",
    "        \n",
    "        return all_valid\n",
    "\n",
    "\n",
    "# Execute validation\n",
    "validator = VariableValidator(CONFIG)\n",
    "validation_summaries = validator.run_validation(file_inventory)\n",
    "all_valid = validator.print_validation_report(validation_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033d1f7",
   "metadata": {},
   "source": [
    "## Stage 2.1: Corrupted File Analysis\n",
    "\n",
    "The validation identified **4 files** (0.002% of total) that cannot be read by xarray's NetCDF backends. This typically indicates:\n",
    "- Corrupted files during download/transfer\n",
    "- Incomplete write operations\n",
    "- File system errors\n",
    "\n",
    "### Action Plan\n",
    "1. Investigate the corrupted files using low-level diagnostics\n",
    "2. Log these files for exclusion from extraction\n",
    "3. Proceed with valid files only (179,872 files)\n",
    "\n",
    "### Affected Files\n",
    "| File | Year | Month | Orbit |\n",
    "|------|------|-------|-------|\n",
    "| `CS_OFFL_SIR_TDP_SI_ANTARC_20101128T231145_20101128T231416_02_05014_C001.nc` | 2010 | 11 | 05014 |\n",
    "| `CS_OFFL_SIR_TDP_SI_ANTARC_20101221T084100_20101221T084146_02_05339_C001.nc` | 2010 | 12 | 05339 |\n",
    "| `CS_OFFL_SIR_TDP_SI_ANTARC_20110226T143225_20110226T143344_03_00971_C001.nc` | 2011 | 02 | 00971 |\n",
    "| `CS_OFFL_SIR_TDP_SI_ANTARC_20110226T143448_20110226T143510_03_00971_C001.nc` | 2011 | 02 | 00971 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1686b6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  CORRUPTED FILE ANALYSIS REPORT\n",
      "================================================================================\n",
      "\n",
      "  Total Corrupted Files: 4\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  [1] CS_OFFL_SIR_TDP_SI_ANTARC_20101128T231145_20101128T231416_02_05014_C001.nc\n",
      "      Path: D:\\phd\\data\\CryoTEMPO\\2010\\11\\CS_OFFL_SIR_TDP_SI_ANTARC_20101128T231145_20101128T231416_02_05014_C001.nc\n",
      "      Size: 0 bytes\n",
      "      Empty: True\n",
      "      Valid NC Signature: False\n",
      "      Diagnosis: Empty or truncated\n",
      "\n",
      "  [2] CS_OFFL_SIR_TDP_SI_ANTARC_20101221T084100_20101221T084146_02_05339_C001.nc\n",
      "      Path: D:\\phd\\data\\CryoTEMPO\\2010\\12\\CS_OFFL_SIR_TDP_SI_ANTARC_20101221T084100_20101221T084146_02_05339_C001.nc\n",
      "      Size: 0 bytes\n",
      "      Empty: True\n",
      "      Valid NC Signature: False\n",
      "      Diagnosis: Empty or truncated\n",
      "\n",
      "  [3] CS_OFFL_SIR_TDP_SI_ANTARC_20110226T143225_20110226T143344_03_00971_C001.nc\n",
      "      Path: D:\\phd\\data\\CryoTEMPO\\2011\\02\\CS_OFFL_SIR_TDP_SI_ANTARC_20110226T143225_20110226T143344_03_00971_C001.nc\n",
      "      Size: 0 bytes\n",
      "      Empty: True\n",
      "      Valid NC Signature: False\n",
      "      Diagnosis: Empty or truncated\n",
      "\n",
      "  [4] CS_OFFL_SIR_TDP_SI_ANTARC_20110226T143448_20110226T143510_03_00971_C001.nc\n",
      "      Path: D:\\phd\\data\\CryoTEMPO\\2011\\02\\CS_OFFL_SIR_TDP_SI_ANTARC_20110226T143448_20110226T143510_03_00971_C001.nc\n",
      "      Size: 0 bytes\n",
      "      Empty: True\n",
      "      Valid NC Signature: False\n",
      "      Diagnosis: Empty or truncated\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "  RECOMMENDATION: Exclude these files from extraction.\n",
      "  These files should be re-downloaded from the source if needed.\n",
      "================================================================================\n",
      "\n",
      "  Files to exclude: 4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stage 2.1: Corrupted File Investigation\n",
    "Diagnose and document corrupted NetCDF files for exclusion.\n",
    "\"\"\"\n",
    "\n",
    "import struct\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class CorruptedFileInfo(NamedTuple):\n",
    "    \"\"\"Information about a corrupted file.\"\"\"\n",
    "    file_path: str\n",
    "    file_name: str\n",
    "    file_size_bytes: int\n",
    "    is_empty: bool\n",
    "    has_nc_signature: bool\n",
    "    error_type: str\n",
    "\n",
    "\n",
    "class CorruptedFileAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzer for diagnosing corrupted NetCDF files.\n",
    "    \n",
    "    This class provides low-level file inspection to understand\n",
    "    why certain files cannot be read by standard NetCDF libraries.\n",
    "    \"\"\"\n",
    "    \n",
    "    # NetCDF-4/HDF5 magic number\n",
    "    NC4_SIGNATURE = b'\\x89HDF\\r\\n\\x1a\\n'\n",
    "    # NetCDF-3 (classic) magic number\n",
    "    NC3_SIGNATURE = b'CDF'\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the analyzer.\"\"\"\n",
    "        self.corrupted_files: List[CorruptedFileInfo] = []\n",
    "    \n",
    "    def check_file_signature(self, file_path: str) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Check if file has valid NetCDF signature.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the file.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (is_valid, format_detected).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                header = f.read(8)\n",
    "                \n",
    "                if len(header) < 3:\n",
    "                    return False, \"Empty or truncated\"\n",
    "                \n",
    "                if header[:3] == self.NC3_SIGNATURE:\n",
    "                    return True, \"NetCDF-3 (Classic)\"\n",
    "                \n",
    "                if header == self.NC4_SIGNATURE:\n",
    "                    return True, \"NetCDF-4 (HDF5)\"\n",
    "                \n",
    "                return False, f\"Unknown format (header: {header[:8].hex()})\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return False, f\"Read error: {str(e)}\"\n",
    "    \n",
    "    def analyze_file(self, file_path: str) -> CorruptedFileInfo:\n",
    "        \"\"\"\n",
    "        Perform comprehensive analysis of a potentially corrupted file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the file.\n",
    "            \n",
    "        Returns:\n",
    "            CorruptedFileInfo with diagnostic information.\n",
    "        \"\"\"\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        try:\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            is_empty = file_size == 0\n",
    "            has_signature, error_type = self.check_file_signature(file_path)\n",
    "            \n",
    "            return CorruptedFileInfo(\n",
    "                file_path=file_path,\n",
    "                file_name=file_name,\n",
    "                file_size_bytes=file_size,\n",
    "                is_empty=is_empty,\n",
    "                has_nc_signature=has_signature,\n",
    "                error_type=error_type\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return CorruptedFileInfo(\n",
    "                file_path=file_path,\n",
    "                file_name=file_name,\n",
    "                file_size_bytes=-1,\n",
    "                is_empty=True,\n",
    "                has_nc_signature=False,\n",
    "                error_type=f\"Access error: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def analyze_corrupted_files(\n",
    "        self, \n",
    "        validation_results: List[FileValidationResult]\n",
    "    ) -> List[CorruptedFileInfo]:\n",
    "        \"\"\"\n",
    "        Analyze all files that failed validation.\n",
    "        \n",
    "        Args:\n",
    "            validation_results: List of validation results.\n",
    "            \n",
    "        Returns:\n",
    "            List of CorruptedFileInfo for failed files.\n",
    "        \"\"\"\n",
    "        failed_files = [\n",
    "            r for r in validation_results \n",
    "            if not r.is_valid and r.error_message is not None\n",
    "        ]\n",
    "        \n",
    "        self.corrupted_files = []\n",
    "        for result in failed_files:\n",
    "            info = self.analyze_file(result.file_path)\n",
    "            self.corrupted_files.append(info)\n",
    "        \n",
    "        return self.corrupted_files\n",
    "    \n",
    "    def print_analysis_report(self) -> None:\n",
    "        \"\"\"Print detailed analysis report for corrupted files.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"  CORRUPTED FILE ANALYSIS REPORT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        if not self.corrupted_files:\n",
    "            print(\"\\n  ✓ No corrupted files detected.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n  Total Corrupted Files: {len(self.corrupted_files)}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, info in enumerate(self.corrupted_files, 1):\n",
    "            print(f\"\\n  [{i}] {info.file_name}\")\n",
    "            print(f\"      Path: {info.file_path}\")\n",
    "            print(f\"      Size: {info.file_size_bytes:,} bytes\")\n",
    "            print(f\"      Empty: {info.is_empty}\")\n",
    "            print(f\"      Valid NC Signature: {info.has_nc_signature}\")\n",
    "            print(f\"      Diagnosis: {info.error_type}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"  RECOMMENDATION: Exclude these files from extraction.\")\n",
    "        print(\"  These files should be re-downloaded from the source if needed.\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def get_exclusion_list(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get list of file paths to exclude from extraction.\n",
    "        \n",
    "        Returns:\n",
    "            List of absolute file paths to exclude.\n",
    "        \"\"\"\n",
    "        return [info.file_path for info in self.corrupted_files]\n",
    "\n",
    "\n",
    "# Analyze corrupted files\n",
    "analyzer = CorruptedFileAnalyzer()\n",
    "corrupted_files_info = analyzer.analyze_corrupted_files(validator.validation_results)\n",
    "analyzer.print_analysis_report()\n",
    "\n",
    "# Store exclusion list for extraction stage\n",
    "EXCLUSION_LIST = analyzer.get_exclusion_list()\n",
    "print(f\"\\n  Files to exclude: {len(EXCLUSION_LIST)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
